{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b92075",
   "metadata": {},
   "source": [
    "# Pig Latin, again\n",
    "\n",
    "Inspired by CSC413 assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6094a1",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef230b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\24608\\AppData\\Local\\Temp\\ipykernel_46040\\358733498.py:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  with open('data\\pig_latin_small.txt','r') as f, \\\n",
      "C:\\Users\\24608\\AppData\\Local\\Temp\\ipykernel_46040\\358733498.py:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  open('data\\small_src.txt','w') as src,\\\n",
      "C:\\Users\\24608\\AppData\\Local\\Temp\\ipykernel_46040\\358733498.py:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  open('data\\small_tgt.txt','w') as tgt:\n",
      "C:\\Users\\24608\\AppData\\Local\\Temp\\ipykernel_46040\\358733498.py:16: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  with open('data\\pig_latin_large.txt','r') as f, \\\n",
      "C:\\Users\\24608\\AppData\\Local\\Temp\\ipykernel_46040\\358733498.py:17: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  open('data\\large_src.txt','w') as src,\\\n",
      "C:\\Users\\24608\\AppData\\Local\\Temp\\ipykernel_46040\\358733498.py:18: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  open('data\\large_tgt.txt','w') as tgt:\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "with open('data\\pig_latin_small.txt','r') as f, \\\n",
    "     open('data\\small_src.txt','w') as src,\\\n",
    "     open('data\\small_tgt.txt','w') as tgt:\n",
    "     for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >=2:\n",
    "                src.write(parts[0] + '\\n')\n",
    "                tgt.write(parts[1] + '\\n')\n",
    "\n",
    "\n",
    "with open('data\\pig_latin_large.txt','r') as f, \\\n",
    "     open('data\\large_src.txt','w') as src,\\\n",
    "     open('data\\large_tgt.txt','w') as tgt:\n",
    "     for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                src.write(parts[0] + '\\n')\n",
    "                tgt.write(parts[1] + '\\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d82102",
   "metadata": {},
   "source": [
    "## Construct Basic Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1289d4d",
   "metadata": {},
   "source": [
    "### Scaled Dot Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c3a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Scaled_Dot_Attention(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(Scaled_Dot_Attention, self).__init__()\n",
    "        self.d = d\n",
    "        self.Q = nn.Linear(d, d)\n",
    "        self.K = nn.Linear(d, d)\n",
    "        self.V = nn.Linear(d, d)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.scaling_factor = torch.rsqrt(\n",
    "            torch.tensor(self.d, dtype=torch.float)\n",
    "        )\n",
    "    def forward(self, queries, keys, values):\n",
    "        Q = self.Q(queries)\n",
    "        K = self.K(keys).transpose(1, 2)\n",
    "        V = self.V(values)\n",
    "        attention_scores = torch.bmm(Q, K) * self.scaling_factor\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        attention = torch.bmm(attention_weights, V)\n",
    "        return attention, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b36fcb2",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc530381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Multihead_Attention(nn.Module):\n",
    "    def __init__(self, d_model, h):\n",
    "        super(Multihead_Attention, self).__init__()\n",
    "        self.d = d_model//h\n",
    "        self.Q = nn.ModuleList()\n",
    "        self.K = nn.ModuleList()\n",
    "        self.V = nn.ModuleList()\n",
    "        self.WO = nn.Linear(h*self.d, d_model)\n",
    "        self.h = h\n",
    "        for i in range(h):\n",
    "            self.Q.append(nn.Linear(d_model, self.d))\n",
    "            self.K.append(nn.Linear(d_model, self.d))\n",
    "            self.V.append(nn.Linear(d_model, self.d))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.scaling_factor = torch.rsqrt(\n",
    "            torch.tensor(self.d, dtype=torch.float)\n",
    "        )\n",
    "    def forward(self, queries, keys, values):\n",
    "        attention_heads = []\n",
    "        attention_weights = []\n",
    "        for i in range(self.h):\n",
    "            Q = self.Q[i](queries)\n",
    "            K = self.K[i](keys).transpose(1, 2)\n",
    "            V = self.V[i](values)\n",
    "            attention_scores = torch.bmm(Q, K) * self.scaling_factor\n",
    "            attention_w = self.softmax(attention_scores)\n",
    "            attention_weights.append(attention_w)\n",
    "            attention = torch.bmm(attention_w, V)\n",
    "            attention_heads.append(attention)\n",
    "        \n",
    "        attention = torch.cat(attention_heads, dim=2)\n",
    "        attention = self.WO(attention)\n",
    "        return attention, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e503c",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7264fd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.0173e-01, -7.3404e-02, -1.1476e-01,  ...,  2.5383e-01,\n",
       "           -1.2380e-01, -3.0524e-02],\n",
       "          [-1.4823e-01, -5.5232e-02, -8.5166e-02,  ...,  2.7197e-01,\n",
       "           -2.3911e-02,  7.8952e-02],\n",
       "          [-1.5832e-01, -6.5937e-02, -5.7866e-02,  ...,  2.2423e-01,\n",
       "           -4.0030e-02,  1.8797e-03],\n",
       "          [-1.7705e-01, -7.8383e-02, -6.4205e-02,  ...,  2.8287e-01,\n",
       "           -9.7089e-02, -8.6408e-03],\n",
       "          [-1.5542e-01, -7.7695e-02, -1.2241e-01,  ...,  2.5611e-01,\n",
       "           -1.5427e-01,  7.1391e-02],\n",
       "          [-1.3411e-01,  1.4788e-03, -6.6327e-02,  ...,  2.7231e-01,\n",
       "           -7.7905e-02,  1.6624e-02]],\n",
       " \n",
       "         [[-7.2817e-02,  2.0150e-01,  6.0139e-02,  ...,  7.8401e-02,\n",
       "            6.5397e-02, -1.1457e-01],\n",
       "          [-8.6510e-02,  2.1970e-01,  5.5662e-02,  ...,  1.3764e-01,\n",
       "            6.9616e-02, -1.8051e-01],\n",
       "          [-1.6608e-01,  1.9520e-01,  7.2893e-02,  ...,  7.2950e-02,\n",
       "            6.7888e-03, -9.5728e-02],\n",
       "          [-1.3589e-01,  2.5303e-01, -1.2464e-02,  ...,  5.9342e-02,\n",
       "            1.9770e-02, -9.2918e-02],\n",
       "          [-6.5557e-02,  2.1956e-01,  3.8878e-02,  ...,  6.8997e-02,\n",
       "           -7.6770e-03, -1.3672e-01],\n",
       "          [-6.7766e-02,  2.0586e-01,  9.4560e-02,  ...,  1.8382e-01,\n",
       "            2.9336e-02, -1.2052e-01]],\n",
       " \n",
       "         [[ 3.4985e-02,  2.2803e-01, -9.4267e-03,  ..., -7.9224e-02,\n",
       "           -1.0581e-01, -1.2598e-01],\n",
       "          [ 1.3654e-02,  2.5765e-01,  5.8221e-02,  ..., -4.5890e-02,\n",
       "           -7.3616e-02, -1.0631e-01],\n",
       "          [-1.6529e-02,  2.4421e-01,  4.5096e-02,  ..., -7.2506e-02,\n",
       "           -4.3949e-02, -5.7108e-02],\n",
       "          [ 1.7314e-02,  2.1031e-01,  1.9294e-02,  ..., -9.9261e-02,\n",
       "           -1.6441e-02, -3.6404e-02],\n",
       "          [-6.9300e-02,  1.8127e-01,  4.0079e-02,  ..., -1.3995e-01,\n",
       "            1.7745e-02, -8.6202e-02],\n",
       "          [ 6.2431e-02,  2.6588e-01, -3.8480e-02,  ..., -2.5725e-02,\n",
       "           -7.9655e-02, -9.4726e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.8826e-03, -1.8035e-01, -1.5049e-01,  ...,  2.1202e-01,\n",
       "            1.9041e-01, -1.3563e-04],\n",
       "          [ 7.1729e-02, -2.1029e-01, -1.6818e-01,  ...,  2.1309e-01,\n",
       "            1.8357e-01,  8.1607e-03],\n",
       "          [ 2.6030e-02, -1.9309e-01, -1.2871e-01,  ...,  1.1532e-01,\n",
       "            1.0553e-01, -1.5028e-02],\n",
       "          [-4.3925e-03, -1.5399e-01, -1.8515e-01,  ...,  1.4262e-01,\n",
       "            1.1370e-01, -3.0311e-02],\n",
       "          [ 3.3544e-02, -2.1999e-01, -1.0411e-01,  ...,  2.2282e-01,\n",
       "            8.6343e-02, -1.3618e-01],\n",
       "          [ 6.6191e-02, -1.6059e-01, -1.3829e-01,  ...,  1.7337e-01,\n",
       "            1.6445e-01, -3.1809e-02]],\n",
       " \n",
       "         [[-2.7731e-02, -1.5869e-01, -5.3164e-02,  ...,  8.4707e-02,\n",
       "            5.4819e-02,  4.0860e-02],\n",
       "          [-2.2127e-02, -1.7214e-01, -3.1263e-02,  ...,  5.8770e-02,\n",
       "           -1.8337e-02, -4.1974e-02],\n",
       "          [-1.0603e-01, -1.1833e-01, -1.1803e-01,  ...,  7.2403e-02,\n",
       "           -3.7591e-02,  5.6846e-02],\n",
       "          [ 2.7575e-02, -1.2359e-01, -1.1882e-01,  ...,  8.4567e-02,\n",
       "           -3.8310e-02,  2.2826e-02],\n",
       "          [-6.9637e-02, -1.5657e-01,  1.4294e-03,  ...,  1.2718e-01,\n",
       "           -5.2047e-02, -1.8494e-02],\n",
       "          [-9.2568e-02, -1.5113e-01, -1.0317e-01,  ...,  1.4496e-01,\n",
       "           -5.5304e-02,  2.2564e-02]],\n",
       " \n",
       "         [[ 8.1498e-02, -2.7683e-02,  3.1491e-01,  ..., -1.3249e-01,\n",
       "            1.5436e-01, -1.0354e-01],\n",
       "          [ 3.3780e-02, -6.5976e-02,  3.1071e-01,  ..., -8.1410e-02,\n",
       "            1.9417e-01, -6.4781e-02],\n",
       "          [ 5.3190e-02, -4.5035e-03,  3.4923e-01,  ..., -1.4304e-01,\n",
       "            1.9188e-01, -6.5278e-02],\n",
       "          [-5.6228e-02,  2.4765e-02,  2.3020e-01,  ..., -1.7193e-01,\n",
       "            1.0864e-01, -4.5136e-02],\n",
       "          [ 5.9447e-02, -3.1731e-02,  3.0396e-01,  ..., -2.1963e-01,\n",
       "            1.3678e-01, -4.1617e-03],\n",
       "          [ 6.4579e-02, -1.7133e-02,  3.0534e-01,  ..., -1.2968e-01,\n",
       "            1.4067e-01, -3.6904e-02]]], grad_fn=<ViewBackward0>),\n",
       " [tensor([[[0.1582, 0.1905, 0.2101, 0.1093, 0.1519, 0.1800],\n",
       "           [0.0937, 0.2227, 0.0828, 0.1345, 0.2107, 0.2557],\n",
       "           [0.1357, 0.1595, 0.2632, 0.1265, 0.1165, 0.1986],\n",
       "           [0.1932, 0.1666, 0.1640, 0.1451, 0.1850, 0.1460],\n",
       "           [0.1208, 0.1853, 0.1518, 0.1272, 0.2743, 0.1407],\n",
       "           [0.2055, 0.1524, 0.1987, 0.1773, 0.1197, 0.1463]],\n",
       "  \n",
       "          [[0.1479, 0.1488, 0.1802, 0.1295, 0.2879, 0.1057],\n",
       "           [0.1403, 0.1104, 0.1611, 0.1316, 0.1795, 0.2770],\n",
       "           [0.2042, 0.2389, 0.1210, 0.1192, 0.2026, 0.1141],\n",
       "           [0.1493, 0.1126, 0.2302, 0.2005, 0.2051, 0.1022],\n",
       "           [0.1550, 0.2179, 0.1367, 0.0930, 0.2393, 0.1581],\n",
       "           [0.1226, 0.3368, 0.1170, 0.1233, 0.0866, 0.2138]],\n",
       "  \n",
       "          [[0.2718, 0.1195, 0.1386, 0.1477, 0.0922, 0.2303],\n",
       "           [0.1975, 0.1506, 0.1761, 0.1387, 0.1796, 0.1574],\n",
       "           [0.2246, 0.1833, 0.1157, 0.1002, 0.2039, 0.1723],\n",
       "           [0.2069, 0.1112, 0.1338, 0.1825, 0.1797, 0.1860],\n",
       "           [0.1637, 0.1461, 0.2158, 0.1743, 0.1507, 0.1493],\n",
       "           [0.2004, 0.1731, 0.1033, 0.1960, 0.1144, 0.2128]],\n",
       "  \n",
       "          [[0.1630, 0.0800, 0.1864, 0.2494, 0.1805, 0.1407],\n",
       "           [0.0951, 0.1687, 0.1610, 0.2135, 0.2101, 0.1517],\n",
       "           [0.1112, 0.2532, 0.1092, 0.1432, 0.1294, 0.2539],\n",
       "           [0.1452, 0.1354, 0.1977, 0.2551, 0.1441, 0.1226],\n",
       "           [0.2927, 0.1394, 0.1324, 0.1448, 0.1596, 0.1311],\n",
       "           [0.0853, 0.1871, 0.1038, 0.2212, 0.1302, 0.2725]],\n",
       "  \n",
       "          [[0.1353, 0.2658, 0.0753, 0.1439, 0.1132, 0.2666],\n",
       "           [0.1176, 0.2658, 0.1391, 0.1327, 0.0922, 0.2525],\n",
       "           [0.1123, 0.1908, 0.1802, 0.1927, 0.1759, 0.1481],\n",
       "           [0.2088, 0.0960, 0.1394, 0.2400, 0.1391, 0.1767],\n",
       "           [0.2079, 0.2206, 0.1275, 0.1764, 0.2223, 0.0453],\n",
       "           [0.1651, 0.1992, 0.1793, 0.1520, 0.2194, 0.0849]],\n",
       "  \n",
       "          [[0.1471, 0.1696, 0.1297, 0.1749, 0.2045, 0.1741],\n",
       "           [0.2859, 0.1071, 0.0880, 0.0987, 0.3229, 0.0975],\n",
       "           [0.0952, 0.1405, 0.2439, 0.1178, 0.1696, 0.2330],\n",
       "           [0.2259, 0.1087, 0.1351, 0.1754, 0.2505, 0.1044],\n",
       "           [0.1051, 0.1193, 0.1635, 0.2261, 0.1955, 0.1906],\n",
       "           [0.1363, 0.2025, 0.1955, 0.1240, 0.1294, 0.2123]],\n",
       "  \n",
       "          [[0.1113, 0.1528, 0.1854, 0.2569, 0.1951, 0.0985],\n",
       "           [0.2966, 0.1141, 0.1174, 0.1783, 0.1274, 0.1663],\n",
       "           [0.2146, 0.1699, 0.1739, 0.2343, 0.1022, 0.1052],\n",
       "           [0.1742, 0.1895, 0.1882, 0.1950, 0.1162, 0.1368],\n",
       "           [0.1252, 0.2261, 0.0937, 0.1667, 0.2181, 0.1703],\n",
       "           [0.1343, 0.1335, 0.1453, 0.2438, 0.1795, 0.1636]],\n",
       "  \n",
       "          [[0.2160, 0.1016, 0.1337, 0.1768, 0.1844, 0.1874],\n",
       "           [0.2124, 0.1957, 0.1830, 0.1743, 0.1071, 0.1275],\n",
       "           [0.1460, 0.1276, 0.1614, 0.1745, 0.2055, 0.1850],\n",
       "           [0.2649, 0.1284, 0.1684, 0.1160, 0.1965, 0.1258],\n",
       "           [0.1847, 0.1179, 0.1950, 0.1889, 0.2072, 0.1063],\n",
       "           [0.2240, 0.2043, 0.1227, 0.1674, 0.1664, 0.1153]],\n",
       "  \n",
       "          [[0.1682, 0.1818, 0.1103, 0.1939, 0.1538, 0.1920],\n",
       "           [0.1289, 0.1554, 0.1777, 0.1577, 0.2204, 0.1599],\n",
       "           [0.1041, 0.2265, 0.1210, 0.1529, 0.1457, 0.2498],\n",
       "           [0.1502, 0.2055, 0.1578, 0.1886, 0.1197, 0.1782],\n",
       "           [0.3088, 0.1177, 0.1009, 0.1450, 0.1430, 0.1845],\n",
       "           [0.1253, 0.1834, 0.1591, 0.2597, 0.1796, 0.0930]],\n",
       "  \n",
       "          [[0.1919, 0.1648, 0.0903, 0.1728, 0.2237, 0.1565],\n",
       "           [0.1106, 0.1652, 0.1763, 0.1667, 0.1759, 0.2053],\n",
       "           [0.1676, 0.1594, 0.1696, 0.1579, 0.2063, 0.1392],\n",
       "           [0.1668, 0.1929, 0.1408, 0.1922, 0.1798, 0.1275],\n",
       "           [0.2336, 0.1990, 0.1771, 0.1573, 0.1103, 0.1227],\n",
       "           [0.2119, 0.1796, 0.1313, 0.1053, 0.2370, 0.1349]]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[[0.1914, 0.1721, 0.1241, 0.1859, 0.1037, 0.2229],\n",
       "           [0.1466, 0.1503, 0.1771, 0.1749, 0.1395, 0.2116],\n",
       "           [0.2036, 0.1976, 0.2007, 0.0765, 0.1600, 0.1616],\n",
       "           [0.1644, 0.1283, 0.1452, 0.1418, 0.2621, 0.1582],\n",
       "           [0.1473, 0.1536, 0.1640, 0.1925, 0.1503, 0.1923],\n",
       "           [0.2149, 0.1601, 0.2536, 0.1078, 0.1117, 0.1519]],\n",
       "  \n",
       "          [[0.2096, 0.1061, 0.2313, 0.1842, 0.1424, 0.1264],\n",
       "           [0.2567, 0.1258, 0.1130, 0.2376, 0.1331, 0.1337],\n",
       "           [0.2425, 0.1016, 0.1440, 0.1508, 0.1233, 0.2378],\n",
       "           [0.1337, 0.2000, 0.2047, 0.1081, 0.1343, 0.2191],\n",
       "           [0.1885, 0.0903, 0.1101, 0.1650, 0.1892, 0.2569],\n",
       "           [0.2061, 0.1433, 0.1149, 0.2368, 0.1484, 0.1504]],\n",
       "  \n",
       "          [[0.1205, 0.2027, 0.1621, 0.1710, 0.1523, 0.1915],\n",
       "           [0.1475, 0.0752, 0.2683, 0.2107, 0.1447, 0.1536],\n",
       "           [0.2073, 0.1855, 0.1611, 0.1310, 0.2112, 0.1039],\n",
       "           [0.1071, 0.0757, 0.1457, 0.2464, 0.0692, 0.3558],\n",
       "           [0.1022, 0.1638, 0.2438, 0.2060, 0.1725, 0.1116],\n",
       "           [0.1967, 0.1643, 0.1248, 0.2448, 0.0999, 0.1695]],\n",
       "  \n",
       "          [[0.0913, 0.1389, 0.1934, 0.2484, 0.1142, 0.2138],\n",
       "           [0.1295, 0.2891, 0.0855, 0.1522, 0.1375, 0.2063],\n",
       "           [0.0930, 0.1703, 0.2540, 0.1981, 0.1660, 0.1186],\n",
       "           [0.2660, 0.1707, 0.1129, 0.1794, 0.1473, 0.1238],\n",
       "           [0.1847, 0.1211, 0.1171, 0.2161, 0.1195, 0.2414],\n",
       "           [0.2448, 0.0963, 0.1364, 0.1223, 0.2591, 0.1412]],\n",
       "  \n",
       "          [[0.1866, 0.1745, 0.1893, 0.1180, 0.1790, 0.1526],\n",
       "           [0.0840, 0.1529, 0.2825, 0.1261, 0.2127, 0.1418],\n",
       "           [0.1507, 0.1960, 0.1714, 0.1882, 0.1982, 0.0955],\n",
       "           [0.1626, 0.2235, 0.1881, 0.1135, 0.2187, 0.0937],\n",
       "           [0.2260, 0.1513, 0.1355, 0.1398, 0.2456, 0.1019],\n",
       "           [0.0857, 0.1435, 0.2711, 0.1743, 0.1369, 0.1885]],\n",
       "  \n",
       "          [[0.1505, 0.1831, 0.1237, 0.3014, 0.1430, 0.0983],\n",
       "           [0.1210, 0.1428, 0.1627, 0.1701, 0.0811, 0.3223],\n",
       "           [0.2375, 0.1068, 0.2255, 0.1684, 0.1198, 0.1421],\n",
       "           [0.2053, 0.1975, 0.1994, 0.1402, 0.1426, 0.1149],\n",
       "           [0.1545, 0.1386, 0.2248, 0.0824, 0.2838, 0.1159],\n",
       "           [0.0991, 0.2869, 0.1711, 0.1085, 0.1955, 0.1389]],\n",
       "  \n",
       "          [[0.2198, 0.0842, 0.0798, 0.1612, 0.2270, 0.2279],\n",
       "           [0.1964, 0.1668, 0.1649, 0.1311, 0.1585, 0.1824],\n",
       "           [0.3005, 0.1771, 0.1534, 0.1158, 0.0639, 0.1894],\n",
       "           [0.1815, 0.1432, 0.1512, 0.1905, 0.2135, 0.1202],\n",
       "           [0.2506, 0.1388, 0.2232, 0.1418, 0.1438, 0.1017],\n",
       "           [0.1001, 0.2296, 0.2995, 0.1124, 0.0942, 0.1641]],\n",
       "  \n",
       "          [[0.1420, 0.1164, 0.2084, 0.1645, 0.1444, 0.2243],\n",
       "           [0.1937, 0.1375, 0.2293, 0.1652, 0.1602, 0.1141],\n",
       "           [0.1906, 0.1306, 0.1221, 0.2000, 0.1816, 0.1751],\n",
       "           [0.1284, 0.2065, 0.1281, 0.1473, 0.1503, 0.2394],\n",
       "           [0.2242, 0.1922, 0.2110, 0.1352, 0.1168, 0.1205],\n",
       "           [0.1424, 0.2679, 0.1801, 0.1371, 0.1440, 0.1286]],\n",
       "  \n",
       "          [[0.2401, 0.1748, 0.1985, 0.1422, 0.1508, 0.0935],\n",
       "           [0.1766, 0.1748, 0.1721, 0.1384, 0.1273, 0.2108],\n",
       "           [0.0998, 0.1672, 0.1525, 0.2191, 0.1679, 0.1935],\n",
       "           [0.1736, 0.1740, 0.1932, 0.1513, 0.1998, 0.1081],\n",
       "           [0.1553, 0.1112, 0.2496, 0.2302, 0.1059, 0.1476],\n",
       "           [0.1772, 0.2309, 0.1036, 0.2068, 0.1349, 0.1466]],\n",
       "  \n",
       "          [[0.1511, 0.1593, 0.1148, 0.2287, 0.1466, 0.1996],\n",
       "           [0.1856, 0.1657, 0.1639, 0.1764, 0.1784, 0.1300],\n",
       "           [0.1960, 0.1169, 0.0829, 0.1626, 0.2099, 0.2317],\n",
       "           [0.0914, 0.2114, 0.1450, 0.1757, 0.1651, 0.2115],\n",
       "           [0.0917, 0.1820, 0.2181, 0.1315, 0.2468, 0.1298],\n",
       "           [0.1363, 0.1420, 0.1382, 0.2255, 0.2532, 0.1049]]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[[0.2573, 0.1760, 0.1710, 0.0875, 0.1479, 0.1603],\n",
       "           [0.1370, 0.1586, 0.1357, 0.1165, 0.2473, 0.2049],\n",
       "           [0.1489, 0.1739, 0.1658, 0.1493, 0.1396, 0.2225],\n",
       "           [0.0936, 0.1454, 0.1955, 0.2189, 0.1659, 0.1807],\n",
       "           [0.0892, 0.1945, 0.2050, 0.3313, 0.0689, 0.1111],\n",
       "           [0.1264, 0.1479, 0.1295, 0.1931, 0.2215, 0.1816]],\n",
       "  \n",
       "          [[0.1585, 0.1458, 0.2006, 0.1367, 0.1920, 0.1664],\n",
       "           [0.1597, 0.1551, 0.2403, 0.1672, 0.1677, 0.1101],\n",
       "           [0.1990, 0.2109, 0.1106, 0.2195, 0.1144, 0.1456],\n",
       "           [0.1569, 0.1455, 0.1238, 0.2612, 0.1118, 0.2009],\n",
       "           [0.1096, 0.1727, 0.1806, 0.1230, 0.2680, 0.1461],\n",
       "           [0.2307, 0.1013, 0.2817, 0.1162, 0.1874, 0.0827]],\n",
       "  \n",
       "          [[0.1471, 0.1508, 0.1314, 0.1535, 0.1423, 0.2750],\n",
       "           [0.1590, 0.1185, 0.1926, 0.1506, 0.1517, 0.2277],\n",
       "           [0.2587, 0.1337, 0.1468, 0.1225, 0.1995, 0.1387],\n",
       "           [0.1826, 0.1421, 0.2022, 0.1227, 0.2615, 0.0889],\n",
       "           [0.1018, 0.0772, 0.1073, 0.4071, 0.1097, 0.1969],\n",
       "           [0.2564, 0.1796, 0.1231, 0.1439, 0.1613, 0.1357]],\n",
       "  \n",
       "          [[0.1290, 0.1226, 0.2509, 0.1752, 0.1883, 0.1340],\n",
       "           [0.3279, 0.0982, 0.1629, 0.1644, 0.1298, 0.1167],\n",
       "           [0.1291, 0.2434, 0.1772, 0.0869, 0.1702, 0.1932],\n",
       "           [0.1705, 0.2183, 0.1363, 0.1368, 0.1441, 0.1941],\n",
       "           [0.1061, 0.1431, 0.2876, 0.1429, 0.1779, 0.1425],\n",
       "           [0.1033, 0.1347, 0.1442, 0.3089, 0.1687, 0.1401]],\n",
       "  \n",
       "          [[0.1903, 0.2104, 0.0802, 0.2375, 0.1484, 0.1332],\n",
       "           [0.1380, 0.1283, 0.2194, 0.1374, 0.2240, 0.1529],\n",
       "           [0.1054, 0.1972, 0.1356, 0.1073, 0.2429, 0.2116],\n",
       "           [0.1359, 0.2188, 0.1501, 0.1947, 0.1703, 0.1302],\n",
       "           [0.1456, 0.0986, 0.1258, 0.2648, 0.1699, 0.1953],\n",
       "           [0.1658, 0.1465, 0.1653, 0.1524, 0.2087, 0.1613]],\n",
       "  \n",
       "          [[0.1608, 0.1497, 0.2047, 0.1980, 0.0846, 0.2022],\n",
       "           [0.1510, 0.1183, 0.2042, 0.2095, 0.2608, 0.0563],\n",
       "           [0.2627, 0.1343, 0.1894, 0.1191, 0.1487, 0.1457],\n",
       "           [0.2249, 0.1680, 0.1840, 0.1252, 0.0985, 0.1994],\n",
       "           [0.2409, 0.1622, 0.1424, 0.0867, 0.2231, 0.1446],\n",
       "           [0.1118, 0.1497, 0.1495, 0.1516, 0.1612, 0.2762]],\n",
       "  \n",
       "          [[0.2516, 0.1331, 0.0806, 0.1323, 0.1654, 0.2370],\n",
       "           [0.1131, 0.1680, 0.1809, 0.1103, 0.1640, 0.2636],\n",
       "           [0.1538, 0.1609, 0.1173, 0.2948, 0.1624, 0.1109],\n",
       "           [0.2939, 0.2278, 0.1428, 0.1469, 0.1050, 0.0835],\n",
       "           [0.3121, 0.1383, 0.0930, 0.1393, 0.1324, 0.1851],\n",
       "           [0.1378, 0.1588, 0.1635, 0.1800, 0.1572, 0.2027]],\n",
       "  \n",
       "          [[0.1115, 0.2672, 0.1639, 0.0907, 0.1316, 0.2351],\n",
       "           [0.1421, 0.2025, 0.1589, 0.0957, 0.2191, 0.1818],\n",
       "           [0.1277, 0.2317, 0.1880, 0.1363, 0.1464, 0.1698],\n",
       "           [0.1518, 0.2685, 0.1449, 0.1548, 0.1889, 0.0910],\n",
       "           [0.1725, 0.1222, 0.1485, 0.1382, 0.1882, 0.2304],\n",
       "           [0.1419, 0.2297, 0.1771, 0.1446, 0.2104, 0.0963]],\n",
       "  \n",
       "          [[0.1813, 0.1664, 0.1780, 0.0881, 0.1311, 0.2551],\n",
       "           [0.2034, 0.1851, 0.1873, 0.1230, 0.1480, 0.1532],\n",
       "           [0.1481, 0.1177, 0.1609, 0.2045, 0.1357, 0.2331],\n",
       "           [0.2452, 0.1378, 0.1822, 0.1609, 0.1560, 0.1179],\n",
       "           [0.1177, 0.1470, 0.1723, 0.1990, 0.1303, 0.2338],\n",
       "           [0.1510, 0.1648, 0.1531, 0.1903, 0.1799, 0.1609]],\n",
       "  \n",
       "          [[0.1731, 0.1051, 0.1608, 0.1635, 0.2206, 0.1768],\n",
       "           [0.1059, 0.1421, 0.2007, 0.1531, 0.1812, 0.2170],\n",
       "           [0.1149, 0.1739, 0.2371, 0.1610, 0.1509, 0.1622],\n",
       "           [0.1279, 0.1418, 0.1803, 0.2199, 0.2036, 0.1265],\n",
       "           [0.3677, 0.1552, 0.1080, 0.1054, 0.1812, 0.0824],\n",
       "           [0.0697, 0.1898, 0.2506, 0.1021, 0.2463, 0.1415]]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[[0.1510, 0.2026, 0.1624, 0.1394, 0.2385, 0.1061],\n",
       "           [0.1325, 0.1642, 0.1424, 0.2216, 0.1785, 0.1608],\n",
       "           [0.3020, 0.0972, 0.1602, 0.2075, 0.1463, 0.0868],\n",
       "           [0.1952, 0.1230, 0.1906, 0.1526, 0.1756, 0.1630],\n",
       "           [0.1345, 0.2108, 0.1566, 0.2184, 0.1156, 0.1641],\n",
       "           [0.1918, 0.1966, 0.1535, 0.1269, 0.2115, 0.1196]],\n",
       "  \n",
       "          [[0.1101, 0.0858, 0.2005, 0.2662, 0.1695, 0.1679],\n",
       "           [0.1808, 0.1382, 0.1954, 0.2420, 0.1455, 0.0981],\n",
       "           [0.1961, 0.1982, 0.1600, 0.1828, 0.1299, 0.1331],\n",
       "           [0.2808, 0.1755, 0.1637, 0.1075, 0.1087, 0.1639],\n",
       "           [0.1534, 0.1360, 0.1111, 0.1924, 0.2016, 0.2056],\n",
       "           [0.1346, 0.1160, 0.2367, 0.1566, 0.2732, 0.0829]],\n",
       "  \n",
       "          [[0.1463, 0.1909, 0.2153, 0.1388, 0.1874, 0.1213],\n",
       "           [0.2366, 0.1322, 0.1304, 0.1313, 0.1781, 0.1914],\n",
       "           [0.1242, 0.1349, 0.2863, 0.2271, 0.1285, 0.0990],\n",
       "           [0.1829, 0.0911, 0.1252, 0.2710, 0.1878, 0.1420],\n",
       "           [0.1142, 0.1486, 0.1344, 0.1790, 0.2274, 0.1965],\n",
       "           [0.2538, 0.1415, 0.1033, 0.1226, 0.1946, 0.1842]],\n",
       "  \n",
       "          [[0.1917, 0.1132, 0.2289, 0.1139, 0.1575, 0.1947],\n",
       "           [0.2080, 0.1913, 0.1180, 0.2197, 0.1417, 0.1214],\n",
       "           [0.2037, 0.0954, 0.0945, 0.2874, 0.2440, 0.0750],\n",
       "           [0.0688, 0.1251, 0.1852, 0.2122, 0.2580, 0.1507],\n",
       "           [0.1002, 0.1206, 0.2429, 0.1741, 0.1642, 0.1980],\n",
       "           [0.2586, 0.1961, 0.1586, 0.1967, 0.0932, 0.0967]],\n",
       "  \n",
       "          [[0.2121, 0.1436, 0.1525, 0.1959, 0.1918, 0.1041],\n",
       "           [0.1383, 0.1355, 0.2940, 0.1057, 0.1168, 0.2097],\n",
       "           [0.1491, 0.1845, 0.1405, 0.1733, 0.1388, 0.2138],\n",
       "           [0.1951, 0.2354, 0.1286, 0.1015, 0.1395, 0.1999],\n",
       "           [0.1335, 0.1953, 0.1316, 0.1418, 0.2099, 0.1878],\n",
       "           [0.1297, 0.0874, 0.1598, 0.2552, 0.1452, 0.2228]],\n",
       "  \n",
       "          [[0.1370, 0.2524, 0.1421, 0.1775, 0.1192, 0.1719],\n",
       "           [0.1618, 0.1459, 0.2317, 0.2007, 0.1480, 0.1118],\n",
       "           [0.1570, 0.2373, 0.1587, 0.1372, 0.1374, 0.1724],\n",
       "           [0.1133, 0.2196, 0.1428, 0.2130, 0.1474, 0.1638],\n",
       "           [0.1300, 0.1461, 0.2091, 0.1242, 0.1638, 0.2268],\n",
       "           [0.2035, 0.1775, 0.2267, 0.1369, 0.1251, 0.1303]],\n",
       "  \n",
       "          [[0.2112, 0.2530, 0.1068, 0.1944, 0.1324, 0.1020],\n",
       "           [0.1239, 0.1531, 0.2054, 0.1659, 0.1528, 0.1989],\n",
       "           [0.1073, 0.1701, 0.1458, 0.2611, 0.1959, 0.1197],\n",
       "           [0.0863, 0.1490, 0.1212, 0.1597, 0.1987, 0.2852],\n",
       "           [0.1851, 0.1419, 0.3167, 0.0896, 0.1434, 0.1232],\n",
       "           [0.1218, 0.2276, 0.1294, 0.2403, 0.0731, 0.2079]],\n",
       "  \n",
       "          [[0.2047, 0.1489, 0.1762, 0.1841, 0.1454, 0.1407],\n",
       "           [0.1362, 0.1861, 0.2412, 0.1470, 0.1703, 0.1192],\n",
       "           [0.1246, 0.3014, 0.1577, 0.1500, 0.0965, 0.1699],\n",
       "           [0.1327, 0.1714, 0.2199, 0.1271, 0.1886, 0.1603],\n",
       "           [0.3584, 0.1490, 0.1521, 0.1302, 0.0989, 0.1114],\n",
       "           [0.1367, 0.1439, 0.1545, 0.1666, 0.2024, 0.1960]],\n",
       "  \n",
       "          [[0.1133, 0.1730, 0.1415, 0.2707, 0.1426, 0.1589],\n",
       "           [0.1411, 0.1518, 0.2282, 0.1151, 0.1729, 0.1908],\n",
       "           [0.1463, 0.2654, 0.1122, 0.1454, 0.1583, 0.1725],\n",
       "           [0.2099, 0.1902, 0.1260, 0.1458, 0.1455, 0.1825],\n",
       "           [0.1523, 0.1749, 0.1128, 0.1676, 0.2381, 0.1543],\n",
       "           [0.1668, 0.1088, 0.2588, 0.1828, 0.1230, 0.1599]],\n",
       "  \n",
       "          [[0.1413, 0.1589, 0.1917, 0.1409, 0.1566, 0.2106],\n",
       "           [0.1216, 0.2000, 0.1351, 0.2220, 0.1493, 0.1721],\n",
       "           [0.1109, 0.1912, 0.2725, 0.1004, 0.1730, 0.1519],\n",
       "           [0.1750, 0.1195, 0.0735, 0.2387, 0.1443, 0.2489],\n",
       "           [0.1529, 0.1522, 0.1656, 0.1153, 0.1452, 0.2688],\n",
       "           [0.1371, 0.2238, 0.0858, 0.1931, 0.1580, 0.2022]]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[[0.1003, 0.1467, 0.1652, 0.1628, 0.1660, 0.2590],\n",
       "           [0.1852, 0.0994, 0.1641, 0.1085, 0.2624, 0.1804],\n",
       "           [0.0927, 0.1538, 0.3285, 0.1443, 0.1098, 0.1709],\n",
       "           [0.1964, 0.1351, 0.2013, 0.1767, 0.1417, 0.1487],\n",
       "           [0.1795, 0.1076, 0.0910, 0.2510, 0.1556, 0.2154],\n",
       "           [0.1162, 0.1336, 0.1751, 0.1928, 0.1936, 0.1886]],\n",
       "  \n",
       "          [[0.0573, 0.2378, 0.1684, 0.1428, 0.0950, 0.2987],\n",
       "           [0.1809, 0.1224, 0.1640, 0.2294, 0.0846, 0.2186],\n",
       "           [0.2072, 0.2237, 0.1202, 0.1735, 0.1356, 0.1398],\n",
       "           [0.1264, 0.1886, 0.0976, 0.1520, 0.2629, 0.1726],\n",
       "           [0.2144, 0.1361, 0.1208, 0.1971, 0.1871, 0.1445],\n",
       "           [0.2778, 0.1060, 0.1087, 0.0778, 0.2076, 0.2221]],\n",
       "  \n",
       "          [[0.1491, 0.2654, 0.1518, 0.1370, 0.1118, 0.1848],\n",
       "           [0.2275, 0.2158, 0.0886, 0.1840, 0.1255, 0.1586],\n",
       "           [0.2015, 0.1029, 0.0992, 0.1500, 0.3242, 0.1222],\n",
       "           [0.1415, 0.1614, 0.2445, 0.1587, 0.1377, 0.1563],\n",
       "           [0.2206, 0.2089, 0.1762, 0.1704, 0.0516, 0.1724],\n",
       "           [0.2254, 0.1972, 0.0945, 0.1332, 0.1622, 0.1874]],\n",
       "  \n",
       "          [[0.2110, 0.1939, 0.2291, 0.1305, 0.0950, 0.1406],\n",
       "           [0.0916, 0.0780, 0.2445, 0.2048, 0.1923, 0.1888],\n",
       "           [0.0865, 0.2617, 0.1809, 0.1402, 0.1297, 0.2009],\n",
       "           [0.2000, 0.2371, 0.1312, 0.1274, 0.1824, 0.1218],\n",
       "           [0.2196, 0.1801, 0.1334, 0.1584, 0.1162, 0.1924],\n",
       "           [0.2994, 0.1483, 0.1818, 0.1567, 0.1107, 0.1031]],\n",
       "  \n",
       "          [[0.1379, 0.1920, 0.1226, 0.2058, 0.1619, 0.1797],\n",
       "           [0.1423, 0.1643, 0.1745, 0.2460, 0.1234, 0.1496],\n",
       "           [0.1705, 0.1945, 0.1179, 0.2021, 0.1648, 0.1502],\n",
       "           [0.0881, 0.2593, 0.1874, 0.1921, 0.1084, 0.1647],\n",
       "           [0.1757, 0.1971, 0.1418, 0.3031, 0.0767, 0.1057],\n",
       "           [0.1289, 0.1720, 0.1880, 0.1421, 0.2331, 0.1359]],\n",
       "  \n",
       "          [[0.1309, 0.1374, 0.1350, 0.1941, 0.2268, 0.1758],\n",
       "           [0.1294, 0.0955, 0.1268, 0.2850, 0.2095, 0.1538],\n",
       "           [0.1934, 0.0977, 0.1813, 0.1739, 0.2207, 0.1330],\n",
       "           [0.0904, 0.1801, 0.1444, 0.1901, 0.1791, 0.2159],\n",
       "           [0.1149, 0.1505, 0.2797, 0.1852, 0.1610, 0.1087],\n",
       "           [0.1625, 0.2572, 0.2042, 0.1018, 0.1711, 0.1032]],\n",
       "  \n",
       "          [[0.1663, 0.1764, 0.1425, 0.0793, 0.2366, 0.1990],\n",
       "           [0.1366, 0.1207, 0.2002, 0.1414, 0.1316, 0.2695],\n",
       "           [0.1272, 0.2251, 0.2003, 0.1379, 0.1881, 0.1215],\n",
       "           [0.1779, 0.1855, 0.1885, 0.2223, 0.1239, 0.1019],\n",
       "           [0.1388, 0.1363, 0.1908, 0.1568, 0.2119, 0.1654],\n",
       "           [0.1267, 0.1878, 0.2695, 0.1347, 0.1415, 0.1400]],\n",
       "  \n",
       "          [[0.1599, 0.3013, 0.1819, 0.0925, 0.1420, 0.1223],\n",
       "           [0.1965, 0.1772, 0.1934, 0.0757, 0.1562, 0.2011],\n",
       "           [0.1408, 0.1079, 0.1846, 0.1306, 0.2664, 0.1697],\n",
       "           [0.1490, 0.1473, 0.1069, 0.2400, 0.1945, 0.1623],\n",
       "           [0.1184, 0.1133, 0.2952, 0.1469, 0.1760, 0.1502],\n",
       "           [0.1320, 0.1947, 0.1856, 0.1580, 0.1239, 0.2057]],\n",
       "  \n",
       "          [[0.1363, 0.1171, 0.1446, 0.2228, 0.2618, 0.1173],\n",
       "           [0.1152, 0.1109, 0.1651, 0.3021, 0.1710, 0.1356],\n",
       "           [0.2782, 0.1408, 0.1490, 0.1237, 0.1534, 0.1549],\n",
       "           [0.1549, 0.1912, 0.1724, 0.2383, 0.1014, 0.1417],\n",
       "           [0.1640, 0.2028, 0.0852, 0.1292, 0.1855, 0.2333],\n",
       "           [0.1464, 0.0894, 0.2679, 0.1248, 0.2312, 0.1402]],\n",
       "  \n",
       "          [[0.2082, 0.1514, 0.1598, 0.1129, 0.1487, 0.2190],\n",
       "           [0.1747, 0.1636, 0.2487, 0.1842, 0.1096, 0.1191],\n",
       "           [0.1693, 0.1414, 0.1514, 0.1766, 0.1005, 0.2608],\n",
       "           [0.1582, 0.2020, 0.2125, 0.1345, 0.1316, 0.1610],\n",
       "           [0.2015, 0.1321, 0.1263, 0.1568, 0.1106, 0.2728],\n",
       "           [0.1715, 0.1251, 0.2012, 0.0872, 0.1678, 0.2472]]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[[0.2437, 0.1978, 0.1932, 0.2003, 0.0928, 0.0722],\n",
       "           [0.1181, 0.2654, 0.1291, 0.0923, 0.1078, 0.2872],\n",
       "           [0.2222, 0.1305, 0.1053, 0.2105, 0.1827, 0.1488],\n",
       "           [0.1830, 0.2210, 0.1850, 0.1550, 0.1149, 0.1411],\n",
       "           [0.1418, 0.2221, 0.2945, 0.1652, 0.0951, 0.0813],\n",
       "           [0.1933, 0.2651, 0.1352, 0.1257, 0.1471, 0.1336]],\n",
       "  \n",
       "          [[0.1835, 0.2062, 0.1544, 0.1430, 0.1599, 0.1529],\n",
       "           [0.2258, 0.1706, 0.1266, 0.1275, 0.0960, 0.2535],\n",
       "           [0.1472, 0.1782, 0.2415, 0.1184, 0.1760, 0.1387],\n",
       "           [0.1255, 0.1625, 0.1856, 0.1242, 0.1386, 0.2636],\n",
       "           [0.1759, 0.2519, 0.2048, 0.0855, 0.1295, 0.1524],\n",
       "           [0.1611, 0.1613, 0.1959, 0.1395, 0.1357, 0.2066]],\n",
       "  \n",
       "          [[0.1361, 0.0946, 0.1494, 0.2418, 0.2361, 0.1420],\n",
       "           [0.1072, 0.2298, 0.2083, 0.1100, 0.1483, 0.1965],\n",
       "           [0.1211, 0.1698, 0.1295, 0.1328, 0.2449, 0.2018],\n",
       "           [0.2232, 0.0904, 0.1694, 0.1672, 0.2774, 0.0726],\n",
       "           [0.2249, 0.1261, 0.2738, 0.1287, 0.1582, 0.0883],\n",
       "           [0.1636, 0.1637, 0.1395, 0.1968, 0.1487, 0.1878]],\n",
       "  \n",
       "          [[0.1772, 0.1372, 0.1299, 0.1777, 0.1869, 0.1911],\n",
       "           [0.1422, 0.1504, 0.2643, 0.2083, 0.0734, 0.1613],\n",
       "           [0.1089, 0.2646, 0.1602, 0.1085, 0.2287, 0.1291],\n",
       "           [0.1561, 0.1449, 0.1841, 0.1435, 0.1153, 0.2561],\n",
       "           [0.2067, 0.0696, 0.1706, 0.2142, 0.1841, 0.1548],\n",
       "           [0.1119, 0.2482, 0.1270, 0.1941, 0.2042, 0.1146]],\n",
       "  \n",
       "          [[0.2520, 0.1241, 0.1481, 0.1015, 0.2389, 0.1354],\n",
       "           [0.0593, 0.1497, 0.1803, 0.1235, 0.3513, 0.1360],\n",
       "           [0.3228, 0.2270, 0.1419, 0.1417, 0.0990, 0.0677],\n",
       "           [0.0917, 0.0923, 0.2944, 0.1407, 0.2253, 0.1555],\n",
       "           [0.1793, 0.1600, 0.1991, 0.1648, 0.1431, 0.1537],\n",
       "           [0.0861, 0.1988, 0.2222, 0.1663, 0.1490, 0.1777]],\n",
       "  \n",
       "          [[0.1479, 0.1565, 0.1271, 0.1337, 0.2282, 0.2065],\n",
       "           [0.1701, 0.1078, 0.3100, 0.1301, 0.1931, 0.0889],\n",
       "           [0.2260, 0.1284, 0.1283, 0.1573, 0.1692, 0.1908],\n",
       "           [0.1825, 0.1821, 0.1678, 0.1814, 0.1133, 0.1729],\n",
       "           [0.1588, 0.2048, 0.1277, 0.2501, 0.1207, 0.1379],\n",
       "           [0.2310, 0.1270, 0.2029, 0.1434, 0.1479, 0.1477]],\n",
       "  \n",
       "          [[0.1784, 0.1408, 0.1703, 0.2499, 0.1371, 0.1235],\n",
       "           [0.1555, 0.2995, 0.1646, 0.1158, 0.1207, 0.1439],\n",
       "           [0.0878, 0.2228, 0.1624, 0.1531, 0.1626, 0.2113],\n",
       "           [0.2696, 0.1425, 0.1192, 0.1524, 0.1478, 0.1684],\n",
       "           [0.2008, 0.1773, 0.1133, 0.2119, 0.1075, 0.1892],\n",
       "           [0.2481, 0.2521, 0.0996, 0.1384, 0.1317, 0.1302]],\n",
       "  \n",
       "          [[0.2600, 0.1255, 0.1481, 0.1859, 0.1236, 0.1569],\n",
       "           [0.1990, 0.3041, 0.1479, 0.1439, 0.1248, 0.0803],\n",
       "           [0.1845, 0.1463, 0.2887, 0.1832, 0.0794, 0.1180],\n",
       "           [0.2094, 0.0898, 0.1625, 0.0856, 0.2105, 0.2422],\n",
       "           [0.1199, 0.1581, 0.2618, 0.2229, 0.1414, 0.0959],\n",
       "           [0.1083, 0.2990, 0.1900, 0.1012, 0.1645, 0.1370]],\n",
       "  \n",
       "          [[0.1314, 0.2022, 0.1295, 0.2097, 0.1909, 0.1363],\n",
       "           [0.0903, 0.1289, 0.1780, 0.2210, 0.2346, 0.1472],\n",
       "           [0.1511, 0.2187, 0.1732, 0.1488, 0.1442, 0.1639],\n",
       "           [0.2637, 0.1699, 0.1802, 0.1090, 0.1041, 0.1731],\n",
       "           [0.1554, 0.1802, 0.1402, 0.1680, 0.1670, 0.1893],\n",
       "           [0.1683, 0.1540, 0.1983, 0.1566, 0.1927, 0.1300]],\n",
       "  \n",
       "          [[0.1300, 0.1197, 0.1459, 0.1990, 0.1685, 0.2370],\n",
       "           [0.1095, 0.1400, 0.1625, 0.1185, 0.2638, 0.2057],\n",
       "           [0.1512, 0.0723, 0.2371, 0.1814, 0.1584, 0.1995],\n",
       "           [0.2576, 0.1295, 0.2136, 0.1360, 0.1330, 0.1303],\n",
       "           [0.2278, 0.1365, 0.1510, 0.1248, 0.2663, 0.0935],\n",
       "           [0.2491, 0.1485, 0.1099, 0.1636, 0.1778, 0.1510]]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[[0.2691, 0.1516, 0.1450, 0.1035, 0.1502, 0.1805],\n",
       "           [0.3110, 0.0809, 0.1287, 0.1657, 0.1309, 0.1827],\n",
       "           [0.1431, 0.1917, 0.1782, 0.1628, 0.1997, 0.1245],\n",
       "           [0.2394, 0.0886, 0.1534, 0.0997, 0.2325, 0.1864],\n",
       "           [0.2251, 0.1922, 0.1791, 0.1476, 0.1312, 0.1248],\n",
       "           [0.1897, 0.1114, 0.1832, 0.1662, 0.1558, 0.1936]],\n",
       "  \n",
       "          [[0.1746, 0.1650, 0.0785, 0.1749, 0.0960, 0.3111],\n",
       "           [0.1925, 0.1957, 0.1703, 0.2445, 0.1004, 0.0967],\n",
       "           [0.1030, 0.1123, 0.1981, 0.2222, 0.2000, 0.1643],\n",
       "           [0.1403, 0.1791, 0.1704, 0.1260, 0.1355, 0.2487],\n",
       "           [0.1428, 0.1220, 0.2064, 0.1423, 0.1623, 0.2241],\n",
       "           [0.1729, 0.1866, 0.1500, 0.1317, 0.1464, 0.2124]],\n",
       "  \n",
       "          [[0.1520, 0.1850, 0.1855, 0.2548, 0.1347, 0.0881],\n",
       "           [0.1697, 0.2396, 0.2039, 0.1052, 0.1642, 0.1174],\n",
       "           [0.1730, 0.2086, 0.2114, 0.1214, 0.1971, 0.0885],\n",
       "           [0.1902, 0.1025, 0.1551, 0.2003, 0.1474, 0.2046],\n",
       "           [0.1327, 0.1277, 0.2056, 0.2075, 0.1863, 0.1403],\n",
       "           [0.1636, 0.2250, 0.2087, 0.1500, 0.0945, 0.1581]],\n",
       "  \n",
       "          [[0.1933, 0.2596, 0.0949, 0.1630, 0.1560, 0.1332],\n",
       "           [0.0782, 0.2206, 0.2767, 0.1396, 0.1653, 0.1195],\n",
       "           [0.1333, 0.2184, 0.0903, 0.1633, 0.2159, 0.1788],\n",
       "           [0.1713, 0.1722, 0.2542, 0.1287, 0.1389, 0.1347],\n",
       "           [0.0717, 0.2216, 0.0832, 0.2365, 0.2119, 0.1751],\n",
       "           [0.3364, 0.0993, 0.0515, 0.1549, 0.1284, 0.2294]],\n",
       "  \n",
       "          [[0.2197, 0.1474, 0.1847, 0.1730, 0.1794, 0.0958],\n",
       "           [0.1245, 0.1567, 0.2136, 0.1457, 0.1811, 0.1783],\n",
       "           [0.1516, 0.1127, 0.2447, 0.2041, 0.1057, 0.1811],\n",
       "           [0.2275, 0.2455, 0.1630, 0.1170, 0.1802, 0.0669],\n",
       "           [0.1901, 0.1025, 0.1581, 0.2041, 0.1230, 0.2223],\n",
       "           [0.1253, 0.1599, 0.2662, 0.1234, 0.1592, 0.1659]],\n",
       "  \n",
       "          [[0.1514, 0.1608, 0.1124, 0.2558, 0.1509, 0.1687],\n",
       "           [0.0907, 0.1758, 0.1188, 0.3187, 0.1424, 0.1536],\n",
       "           [0.1253, 0.1600, 0.1310, 0.1817, 0.1349, 0.2670],\n",
       "           [0.1644, 0.1443, 0.2236, 0.1506, 0.1426, 0.1746],\n",
       "           [0.1297, 0.1717, 0.1656, 0.1773, 0.1919, 0.1638],\n",
       "           [0.1631, 0.2111, 0.1837, 0.2049, 0.1235, 0.1137]],\n",
       "  \n",
       "          [[0.2025, 0.1507, 0.2628, 0.1127, 0.1483, 0.1231],\n",
       "           [0.1564, 0.0951, 0.1200, 0.1899, 0.2642, 0.1744],\n",
       "           [0.1421, 0.1419, 0.2565, 0.1301, 0.1730, 0.1565],\n",
       "           [0.1325, 0.1595, 0.4175, 0.0846, 0.0881, 0.1178],\n",
       "           [0.1729, 0.2413, 0.1391, 0.1305, 0.1679, 0.1483],\n",
       "           [0.1526, 0.1602, 0.1540, 0.1557, 0.1913, 0.1861]],\n",
       "  \n",
       "          [[0.2139, 0.1518, 0.0997, 0.0683, 0.1649, 0.3013],\n",
       "           [0.2138, 0.1143, 0.1674, 0.1827, 0.1204, 0.2014],\n",
       "           [0.1929, 0.1779, 0.2180, 0.1592, 0.0867, 0.1652],\n",
       "           [0.2955, 0.1635, 0.1510, 0.1047, 0.1336, 0.1518],\n",
       "           [0.0704, 0.1577, 0.2527, 0.1824, 0.1712, 0.1656],\n",
       "           [0.2220, 0.1778, 0.1725, 0.1736, 0.1103, 0.1438]],\n",
       "  \n",
       "          [[0.1239, 0.1602, 0.1029, 0.2154, 0.2690, 0.1286],\n",
       "           [0.1311, 0.0894, 0.2523, 0.1200, 0.1761, 0.2311],\n",
       "           [0.1209, 0.2043, 0.1507, 0.2285, 0.1421, 0.1535],\n",
       "           [0.1224, 0.1905, 0.1569, 0.1645, 0.1554, 0.2102],\n",
       "           [0.3306, 0.1220, 0.1468, 0.1126, 0.1423, 0.1456],\n",
       "           [0.2167, 0.1591, 0.1332, 0.1448, 0.2061, 0.1402]],\n",
       "  \n",
       "          [[0.1163, 0.1199, 0.2247, 0.1681, 0.2291, 0.1419],\n",
       "           [0.1467, 0.1996, 0.1090, 0.1477, 0.2546, 0.1423],\n",
       "           [0.1250, 0.2653, 0.1082, 0.1602, 0.1832, 0.1580],\n",
       "           [0.2328, 0.1282, 0.1069, 0.1831, 0.1833, 0.1658],\n",
       "           [0.1392, 0.1510, 0.1381, 0.2174, 0.1036, 0.2508],\n",
       "           [0.2251, 0.2073, 0.1383, 0.1197, 0.1337, 0.1759]]],\n",
       "         grad_fn=<SoftmaxBackward0>),\n",
       "  tensor([[[0.1280, 0.1936, 0.2420, 0.1130, 0.2049, 0.1185],\n",
       "           [0.1468, 0.1317, 0.2025, 0.2003, 0.2201, 0.0987],\n",
       "           [0.1277, 0.2262, 0.1908, 0.1229, 0.1756, 0.1568],\n",
       "           [0.2295, 0.2264, 0.1444, 0.1468, 0.1362, 0.1168],\n",
       "           [0.1839, 0.2162, 0.1653, 0.1274, 0.1512, 0.1560],\n",
       "           [0.1610, 0.1730, 0.1307, 0.2177, 0.1392, 0.1785]],\n",
       "  \n",
       "          [[0.1603, 0.1902, 0.1267, 0.0872, 0.2259, 0.2098],\n",
       "           [0.1083, 0.1319, 0.2130, 0.2040, 0.1282, 0.2147],\n",
       "           [0.1853, 0.2380, 0.1061, 0.1990, 0.0966, 0.1750],\n",
       "           [0.1518, 0.3329, 0.2061, 0.1080, 0.1066, 0.0945],\n",
       "           [0.1977, 0.2157, 0.1681, 0.1098, 0.2118, 0.0970],\n",
       "           [0.0998, 0.2426, 0.1192, 0.0918, 0.2192, 0.2273]],\n",
       "  \n",
       "          [[0.0869, 0.2417, 0.1986, 0.1556, 0.1052, 0.2120],\n",
       "           [0.2467, 0.1165, 0.1629, 0.1748, 0.1523, 0.1468],\n",
       "           [0.2615, 0.0961, 0.1699, 0.1514, 0.1455, 0.1756],\n",
       "           [0.1870, 0.2347, 0.1832, 0.1292, 0.1107, 0.1552],\n",
       "           [0.1212, 0.1387, 0.1509, 0.0884, 0.2400, 0.2608],\n",
       "           [0.1664, 0.2043, 0.1462, 0.2021, 0.1330, 0.1481]],\n",
       "  \n",
       "          [[0.0946, 0.1540, 0.1278, 0.1772, 0.2717, 0.1745],\n",
       "           [0.1397, 0.1596, 0.1248, 0.2278, 0.1591, 0.1889],\n",
       "           [0.2245, 0.2112, 0.1431, 0.1383, 0.1487, 0.1342],\n",
       "           [0.1819, 0.1945, 0.1241, 0.1561, 0.1250, 0.2183],\n",
       "           [0.1711, 0.1433, 0.1101, 0.2431, 0.2270, 0.1055],\n",
       "           [0.1092, 0.2342, 0.0741, 0.2699, 0.1254, 0.1873]],\n",
       "  \n",
       "          [[0.1271, 0.2180, 0.2157, 0.1656, 0.1443, 0.1293],\n",
       "           [0.1386, 0.1538, 0.1532, 0.1911, 0.1221, 0.2412],\n",
       "           [0.1983, 0.1501, 0.1812, 0.1725, 0.1567, 0.1412],\n",
       "           [0.1174, 0.1357, 0.1823, 0.2296, 0.1341, 0.2009],\n",
       "           [0.2406, 0.1558, 0.1648, 0.1994, 0.1094, 0.1300],\n",
       "           [0.1810, 0.1073, 0.1920, 0.1242, 0.1753, 0.2202]],\n",
       "  \n",
       "          [[0.1827, 0.2389, 0.1170, 0.1770, 0.0713, 0.2132],\n",
       "           [0.1225, 0.0962, 0.1375, 0.2141, 0.2315, 0.1982],\n",
       "           [0.1084, 0.1070, 0.1925, 0.2121, 0.1819, 0.1982],\n",
       "           [0.2742, 0.1328, 0.1262, 0.1160, 0.1448, 0.2059],\n",
       "           [0.1109, 0.2849, 0.1730, 0.1883, 0.0835, 0.1594],\n",
       "           [0.2098, 0.1806, 0.1791, 0.1925, 0.1023, 0.1357]],\n",
       "  \n",
       "          [[0.0846, 0.1375, 0.0952, 0.2464, 0.3222, 0.1140],\n",
       "           [0.2602, 0.1000, 0.1575, 0.1279, 0.2357, 0.1186],\n",
       "           [0.1631, 0.1194, 0.1902, 0.1636, 0.1553, 0.2085],\n",
       "           [0.2389, 0.1344, 0.1119, 0.2145, 0.1676, 0.1326],\n",
       "           [0.1386, 0.2736, 0.1438, 0.1450, 0.1242, 0.1748],\n",
       "           [0.0898, 0.1859, 0.3905, 0.0834, 0.1444, 0.1061]],\n",
       "  \n",
       "          [[0.1932, 0.1684, 0.1632, 0.1407, 0.1942, 0.1402],\n",
       "           [0.1981, 0.0833, 0.2398, 0.1032, 0.2949, 0.0807],\n",
       "           [0.1721, 0.1776, 0.1858, 0.1023, 0.2237, 0.1385],\n",
       "           [0.1231, 0.1685, 0.2103, 0.1420, 0.1821, 0.1740],\n",
       "           [0.1001, 0.1501, 0.1130, 0.2930, 0.1406, 0.2032],\n",
       "           [0.1524, 0.1184, 0.2079, 0.1700, 0.2174, 0.1339]],\n",
       "  \n",
       "          [[0.1081, 0.1573, 0.2878, 0.1313, 0.0792, 0.2363],\n",
       "           [0.1523, 0.1415, 0.1504, 0.1578, 0.1806, 0.2174],\n",
       "           [0.1672, 0.1796, 0.1922, 0.1567, 0.1596, 0.1446],\n",
       "           [0.2468, 0.1676, 0.1484, 0.0984, 0.1967, 0.1421],\n",
       "           [0.1608, 0.2672, 0.1319, 0.1762, 0.0867, 0.1772],\n",
       "           [0.2124, 0.1313, 0.2928, 0.1181, 0.1541, 0.0913]],\n",
       "  \n",
       "          [[0.1784, 0.2762, 0.1112, 0.1575, 0.1201, 0.1566],\n",
       "           [0.1007, 0.2825, 0.0912, 0.0684, 0.2591, 0.1981],\n",
       "           [0.1331, 0.0835, 0.1704, 0.2276, 0.2726, 0.1128],\n",
       "           [0.1784, 0.1486, 0.1753, 0.1699, 0.1578, 0.1699],\n",
       "           [0.1350, 0.2055, 0.1568, 0.2039, 0.0971, 0.2018],\n",
       "           [0.1366, 0.2088, 0.1983, 0.1293, 0.1490, 0.1781]]],\n",
       "         grad_fn=<SoftmaxBackward0>)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Multihead_Attention(512, 8)\n",
    "x = torch.randn(10, 6, 512)\n",
    "test.forward(x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9128331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original attention scores:\n",
      " tensor([[[1., 2., 3.],\n",
      "         [4., 5., 6.],\n",
      "         [7., 8., 9.]]])\n",
      "\n",
      "Masked attention scores:\n",
      " tensor([[[ 1.0000e+00, -1.0000e+09,  3.0000e+00],\n",
      "         [ 4.0000e+00,  5.0000e+00, -1.0000e+09],\n",
      "         [-1.0000e+09,  8.0000e+00,  9.0000e+00]]])\n",
      "\n",
      "Attention probabilities after softmax:\n",
      " tensor([[[0.1192, 0.0000, 0.8808],\n",
      "         [0.2689, 0.7311, 0.0000],\n",
      "         [0.0000, 0.2689, 0.7311]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example dummy attention scores (batch_size=1, query_len=3, key_len=3)\n",
    "attention_scores = torch.tensor([[[1.0, 2.0, 3.0],\n",
    "                                  [4.0, 5.0, 6.0],\n",
    "                                  [7.0, 8.0, 9.0]]])\n",
    "\n",
    "print(\"Original attention scores:\\n\", attention_scores)\n",
    "\n",
    "# Create a mask: 1 allows attention, 0 blocks attention\n",
    "mask = torch.tensor([[[1, 0, 1],\n",
    "                      [1, 1, 0],\n",
    "                      [0, 1, 1]]])\n",
    "\n",
    "# Apply mask using masked_fill: wherever mask == 0, set to -1e9 (large negative)\n",
    "masked_attention_scores = attention_scores.masked_fill(mask == 0, float('-1e9'))\n",
    "\n",
    "print(\"\\nMasked attention scores:\\n\", masked_attention_scores)\n",
    "\n",
    "# Apply softmax over last dimension (key_len)\n",
    "attention_probs = F.softmax(masked_attention_scores, dim=-1)\n",
    "\n",
    "print(\"\\nAttention probabilities after softmax:\\n\", attention_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab69cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Multihead_Attention_Masked(nn.Module):\n",
    "    def __init__(self, d_model, h):\n",
    "        super(Multihead_Attention_Masked, self).__init__()\n",
    "        self.d = d_model//h\n",
    "        self.Q = nn.ModuleList()\n",
    "        self.K = nn.ModuleList()\n",
    "        self.V = nn.ModuleList()\n",
    "        self.WO = nn.Linear(h*self.d, d_model)\n",
    "        self.h = h\n",
    "        for i in range(h):\n",
    "            self.Q.append(nn.Linear(d_model, self.d))\n",
    "            self.K.append(nn.Linear(d_model, self.d))\n",
    "            self.V.append(nn.Linear(d_model, self.d))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.scaling_factor = torch.rsqrt(\n",
    "            torch.tensor(self.d, dtype=torch.float)\n",
    "        )\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        attention_heads = []\n",
    "        attention_weights = []\n",
    "        for i in range(self.h):\n",
    "            Q = self.Q[i](queries)\n",
    "            K = self.K[i](keys).transpose(1, 2)\n",
    "            V = self.V[i](values)\n",
    "            attention_scores = torch.bmm(Q, K) * self.scaling_factor\n",
    "\n",
    "            if mask is not None:\n",
    "                attention_scores = attention_scores.masked_fill(mask == 0, float('-1e9'))\n",
    "                \n",
    "            attention_w = self.softmax(attention_scores)\n",
    "            attention_weights.append(attention_w)\n",
    "            attention = torch.bmm(attention_w, V)\n",
    "            attention_heads.append(attention)\n",
    "        \n",
    "        attention = torch.cat(attention_heads, dim=2)\n",
    "        attention = self.WO(attention)\n",
    "        return attention, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84c79b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 32])\n",
      "Attention weights shape per head: torch.Size([2, 5, 5])\n",
      "Causal mask:\n",
      " tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "First head attention weights:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3651, 0.6349, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4139, 0.2928, 0.2933, 0.0000, 0.0000],\n",
      "        [0.1590, 0.2242, 0.3177, 0.2991, 0.0000],\n",
      "        [0.1350, 0.1600, 0.3227, 0.1777, 0.2047]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define your attention module\n",
    "mha = Multihead_Attention_Masked(d_model=32, h=4)  # Example small model\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 32\n",
    "\n",
    "# Create dummy inputs\n",
    "x = torch.randn(batch_size, seq_len, d_model)  # (batch, seq_len, d_model)\n",
    "\n",
    "# Create causal mask (batch_size, seq_len, seq_len)\n",
    "def generate_causal_mask(batch_size, seq_len, device):\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, device=device)).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    return mask\n",
    "\n",
    "mask = generate_causal_mask(batch_size, seq_len, x.device)\n",
    "\n",
    "# Forward pass with mask\n",
    "output, attn_weights = mha(x, x, x, mask=mask)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Expected: (batch_size, seq_len, d_model)\n",
    "print(\"Attention weights shape per head:\", attn_weights[0].shape)  # (batch_size, seq_len, seq_len)\n",
    "print(\"Causal mask:\\n\", mask[0])\n",
    "print(\"First head attention weights:\\n\", attn_weights[0][0])  # Visualize head 0, batch 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52653d26",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "class Transformer_Encoder_naive(nn.Module):\n",
    "    def __init__(self, N, d_model, n_head, vocab_size):\n",
    "        super(Transformer_Encoder_naive, self).__init__()\n",
    "        self.N = N\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.positional_encoding()\n",
    "\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [Multihead_Attention(d_model, n_head) for i in range(N)]\n",
    "        )\n",
    "\n",
    "        self.mlps = nn.ModuleList(\n",
    "            [   nn.Sequential(\n",
    "                    nn.Linear(d_model, 4*d_model),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4*d_model, d_model),\n",
    "                )\n",
    "                for i in range(self.N)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded + self.pe[:seq_len]\n",
    "        for i in range(self.N):\n",
    "            attention, _ = self.attentions[i](embedded, embedded, embedded)\n",
    "            attention = attention + embedded\n",
    "            attention = nn.LayerNorm(attention.shape)(attention)\n",
    "            mlp = self.mlps[i](attention)\n",
    "            embedded = mlp + attention\n",
    "            embedded = nn.LayerNorm(embedded.shape)(embedded)\n",
    "        return embedded\n",
    "\n",
    "    def positional_encoding(self, seq_len = 1000):\n",
    "        d_model = self.d_model\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        exp_term = torch.arange(d_model//2).unsqueeze(0) / d_model\n",
    "        sin_term = torch.sin(position / (10000 ** exp_term))\n",
    "        cos_term = torch.cos(position / (10000 ** exp_term))\n",
    "        positional_encoding = torch.zeros(seq_len, d_model)\n",
    "        positional_encoding[:, 0::2] = sin_term\n",
    "        positional_encoding[:, 1::2] = cos_term\n",
    "        positional_encoding.to('cuda')\n",
    "        return positional_encoding\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e9b0f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"N = 6\n",
    "d_model = 512\n",
    "n_head = 8\n",
    "class Transformer_Encoder(nn.Module):\n",
    "    def __init__(self, N, d_model, n_head, vocab_size):\n",
    "        super(Transformer_Encoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.positional_encoding()\n",
    "        self.pe.to('cuda')\n",
    "\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [Multihead_Attention_Masked(d_model, n_head) for i in range(N)]\n",
    "        )\n",
    "\n",
    "        self.mlps = nn.ModuleList(\n",
    "            [   nn.Sequential(\n",
    "                    nn.Linear(d_model, 4*d_model),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4*d_model, d_model),\n",
    "                )\n",
    "                for i in range(self.N)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.to('cuda')\n",
    "        seq_len = x.shape[1]\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded + self.pe[:seq_len].to(embedded.device)\n",
    "        for i in range(self.N):\n",
    "            attention, _ = self.attentions[i](embedded, embedded, embedded)\n",
    "            attention = attention + embedded\n",
    "            attention = nn.LayerNorm(attention.shape)(attention)\n",
    "            mlp = self.mlps[i](attention)\n",
    "            embedded = mlp + attention\n",
    "            embedded = nn.LayerNorm(embedded.shape)(embedded)\n",
    "        return embedded\n",
    "\n",
    "    def positional_encoding(self, seq_len = 1000):\n",
    "        d_model = self.d_model\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        exp_term = torch.arange(d_model//2).unsqueeze(0) / d_model\n",
    "        sin_term = torch.sin(position / (10000 ** exp_term))\n",
    "        cos_term = torch.cos(position / (10000 ** exp_term))\n",
    "        positional_encoding = torch.zeros(seq_len, d_model)\n",
    "        positional_encoding[:, 0::2] = sin_term\n",
    "        positional_encoding[:, 1::2] = cos_term\n",
    "        return positional_encoding\n",
    "        \"\"\"\n",
    "class Transformer_Encoder(nn.Module):\n",
    "    def __init__(self, N, d_model, n_head, vocab_size):\n",
    "        super(Transformer_Encoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Positional Encoding registered as buffer (device safe)\n",
    "        self.register_buffer('pe', self.positional_encoding())\n",
    "\n",
    "        # Attention modules\n",
    "        self.attentions = nn.ModuleList([\n",
    "            Multihead_Attention_Masked(d_model, n_head) for _ in range(N)\n",
    "        ])\n",
    "\n",
    "        # Feedforward (MLP) modules\n",
    "        self.mlps = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, 4 * d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * d_model, d_model),\n",
    "            ) for _ in range(N)\n",
    "        ])\n",
    "\n",
    "        # LayerNorms: 2 per block (after attn, after mlp)\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model) for _ in range(2 * N)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device  # Get device dynamically\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        # Embedding + positional encoding\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        pe = self.pe[:seq_len, :].to(device)  # Make sure PE is on same device\n",
    "        embedded = embedded + pe\n",
    "\n",
    "        # Encoder layers\n",
    "        for i in range(self.N):\n",
    "            # Self-attention (masked or not, depends on design)\n",
    "            attention, _ = self.attentions[i](embedded, embedded, embedded)\n",
    "            embedded = self.layer_norms[2 * i](embedded + attention)\n",
    "\n",
    "            # Feedforward (MLP)\n",
    "            mlp_output = self.mlps[i](embedded)\n",
    "            embedded = self.layer_norms[2 * i + 1](embedded + mlp_output)\n",
    "\n",
    "        return embedded  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    def positional_encoding(self, seq_len=1000):\n",
    "        d_model = self.d_model\n",
    "        position = torch.arange(seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe  # (seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3be3217a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6070, -0.4748,  0.0108,  ..., -0.1395, -0.5308,  0.1014],\n",
       "         [ 1.6450,  0.1393, -0.8619,  ...,  0.0583, -0.6279,  0.7517],\n",
       "         [ 0.0480, -0.3419,  0.6967,  ...,  0.3572, -1.1321,  0.1739],\n",
       "         [-0.4232, -3.2014, -0.6346,  ...,  0.1541, -0.6211,  0.2021],\n",
       "         [ 0.6266, -0.2097, -1.1710,  ..., -0.2623, -0.0604,  0.4459],\n",
       "         [-0.5671, -0.4261, -0.5070,  ..., -0.1120, -0.8331,  0.8379]],\n",
       "\n",
       "        [[ 0.4379,  0.3000,  0.6080,  ...,  0.8164, -0.3258,  1.1259],\n",
       "         [-0.0790, -1.1930,  0.2627,  ...,  1.0599, -1.9356, -0.0343],\n",
       "         [-0.2700, -1.2963,  0.3298,  ...,  0.6309,  1.8400, -1.2053],\n",
       "         [-0.8559, -0.1037, -0.9344,  ..., -0.3237,  0.7930,  0.0665],\n",
       "         [-0.8746, -1.6965, -1.1340,  ...,  1.5759,  0.8942,  1.0444],\n",
       "         [-1.6168, -0.9863, -0.3203,  ...,  1.0035,  1.2694,  1.2060]],\n",
       "\n",
       "        [[ 0.0726, -1.1976,  0.6546,  ..., -1.0682, -1.2797,  1.5777],\n",
       "         [-0.5232, -0.4907,  1.3249,  ...,  0.2323, -1.4941,  2.0197],\n",
       "         [ 1.0316,  0.5009,  0.2504,  ..., -0.1634, -0.1271, -0.6570],\n",
       "         [ 0.2298, -1.5490, -1.1798,  ...,  0.4255, -1.0555,  0.7434],\n",
       "         [-0.4843, -0.8252, -1.2115,  ...,  0.3720,  0.2199, -0.3859],\n",
       "         [-1.4697,  0.3542, -1.1671,  ..., -0.7383, -0.8796,  1.0216]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.6147,  0.9774,  0.4561,  ..., -0.9566,  0.5465,  0.7737],\n",
       "         [ 1.4744,  0.7679,  0.7462,  ..., -0.6756, -0.4581,  0.7834],\n",
       "         [ 0.2247, -0.6188,  1.0236,  ..., -0.2184, -0.0861,  0.3743],\n",
       "         [-0.1908, -1.8448,  1.6001,  ...,  0.9521,  0.2633,  0.3338],\n",
       "         [ 0.2535, -1.0386, -1.3200,  ..., -0.4359,  0.1986,  2.1724],\n",
       "         [-0.4178,  0.9070, -0.5042,  ...,  0.2932,  1.5029,  0.1608]],\n",
       "\n",
       "        [[ 0.3293,  0.7044,  0.2080,  ...,  0.6732,  0.4635,  1.3966],\n",
       "         [ 0.4198, -0.4984,  0.7891,  ..., -0.0326, -0.2305, -0.3862],\n",
       "         [ 0.7124, -1.1961, -0.6054,  ...,  1.1943,  0.6590, -0.4122],\n",
       "         [ 0.4169, -1.0537,  0.1039,  ..., -0.3628,  1.2236,  0.5536],\n",
       "         [-0.4543, -0.4035,  1.0330,  ...,  0.2668,  1.2275,  0.7818],\n",
       "         [-0.5804,  0.4232, -0.9595,  ...,  0.0268, -0.7550, -0.6731]],\n",
       "\n",
       "        [[ 0.4553,  0.1906,  0.0600,  ...,  0.5474,  0.0950,  1.9035],\n",
       "         [-0.4168,  0.2942, -0.4349,  ..., -0.1878, -0.2962, -0.6232],\n",
       "         [ 0.3910, -1.6117,  1.1312,  ...,  1.0287, -0.8828, -0.0550],\n",
       "         [ 2.4899, -1.1950, -0.6286,  ...,  0.1700, -0.3992, -0.4958],\n",
       "         [-0.1480, -0.0619, -0.1050,  ..., -0.4055,  0.1779,  0.4065],\n",
       "         [-1.9867,  0.0296, -0.7710,  ...,  0.1786,  0.1257, -0.1888]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoder = Transformer_Encoder_naive(N, d_model, n_head, 1000)\n",
    "test_tensor = torch.randint(0, 1000, (10, 6))\n",
    "print(test_tensor.shape)\n",
    "test_encoder.forward(test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a5d87",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3ae7450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer_Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, vocab_size, max_seq_len=1000):\n",
    "        super(Transformer_Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.pe = self.positional_encoding(max_seq_len).to(device='cuda')\n",
    "\n",
    "        self.self_attentions = nn.ModuleList([\n",
    "            Multihead_Attention_Masked(d_model, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.cross_attentions = nn.ModuleList([\n",
    "            Multihead_Attention_Masked(d_model, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.mlps = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, 4 * d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * d_model, d_model)\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model) for _ in range(3 * num_layers) \n",
    "        ])\n",
    "\n",
    "    def forward(self, target_input, encoder_output, mask=None, encoder_mask=None):\n",
    "\n",
    "        seq_len = target_input.size(1)\n",
    "        embedded = self.embedding(target_input) + self.pe[:seq_len, :]\n",
    "\n",
    "        decoder_output = embedded\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            causal_mask = self.generate_causal_mask(decoder_output.size(0), seq_len, decoder_output.device)\n",
    "            self_attn, _ = self.self_attentions[i](decoder_output, decoder_output, decoder_output, mask=causal_mask)\n",
    "            decoder_output = self.layer_norms[i * 3](decoder_output + self_attn)\n",
    "\n",
    "            cross_attn, _ = self.cross_attentions[i](decoder_output, encoder_output, encoder_output, mask=encoder_mask)\n",
    "            decoder_output = self.layer_norms[i * 3 + 1](decoder_output + cross_attn)\n",
    "\n",
    "            mlp_output = self.mlps[i](decoder_output)\n",
    "            decoder_output = self.layer_norms[i * 3 + 2](decoder_output + mlp_output)\n",
    "\n",
    "        return decoder_output  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    def generate_causal_mask(self, batch_size, seq_len, device):\n",
    "        # Generates a lower triangular mask (causal)\n",
    "        return torch.tril(torch.ones(seq_len, seq_len, device=device)).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "    def positional_encoding(self, max_seq_len):\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * -(torch.log(torch.tensor(10000.0)) / self.d_model))\n",
    "        pe = torch.zeros(max_seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d5e29",
   "metadata": {},
   "source": [
    "### Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d226e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src_input, tgt_input, src_mask=None, tgt_mask=None):\n",
    "\n",
    "        encoder_output = self.encoder(src_input)\n",
    "\n",
    "        decoder_output = self.decoder(tgt_input, encoder_output, mask=tgt_mask, encoder_mask=src_mask) \n",
    "\n",
    "        logits = self.output_projection(decoder_output) \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd9c40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([4, 6, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Create encoder and decoder (reuse your naive ones)\n",
    "encoder = Transformer_Encoder(N=6, d_model=512, n_head=8, vocab_size=1000)\n",
    "decoder = Transformer_Decoder(d_model=512, num_heads=8, num_layers=6, vocab_size=1000)\n",
    "\n",
    "# Create wrapper\n",
    "transformer = TransformerEncoderDecoder(encoder, decoder, d_model=512, vocab_size=1000)\n",
    "\n",
    "# Dummy input tokens\n",
    "src_input = torch.randint(0, 1000, (4, 10))  # (batch_size, src_seq_len)\n",
    "tgt_input = torch.randint(0, 1000, (4, 6))   # (batch_size, tgt_seq_len)\n",
    "\n",
    "# Causal mask for decoder (optional but recommended)\n",
    "def generate_causal_mask(batch_size, seq_len, device):\n",
    "    return torch.tril(torch.ones(seq_len, seq_len, device=device)).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "tgt_mask = generate_causal_mask(batch_size=4, seq_len=6, device=src_input.device)\n",
    "\n",
    "# Forward pass\n",
    "logits = transformer(src_input, tgt_input, src_mask=None, tgt_mask=tgt_mask)\n",
    "print(\"Logits shape:\", logits.shape)  # Expected: (batch_size, tgt_seq_len, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256c1f1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d98a578d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3255 samples\n",
      "Example:\n",
      "SRC: apprehensive\n",
      "TGT: apprehensiveway\n",
      "Vocabulary size: 42\n",
      "SRC indices: [1, 16, 31, 31, 33, 20, 23, 20, 29, 34, 24, 37, 20, 2]\n",
      "TGT indices: [1, 16, 31, 31, 33, 20, 23, 20, 29, 34, 24, 37, 20, 38, 16, 40, 2]\n",
      "Epoch 1: Loss = 1.3842\n",
      "Epoch 2: Loss = 0.3071\n",
      "Epoch 3: Loss = 0.1006\n",
      "Epoch 4: Loss = 0.0623\n",
      "Epoch 5: Loss = 0.0429\n",
      "Epoch 6: Loss = 0.0308\n",
      "Epoch 7: Loss = 0.0427\n",
      "Epoch 8: Loss = 0.0363\n",
      "Epoch 9: Loss = 0.0230\n",
      "Epoch 10: Loss = 0.0292\n"
     ]
    }
   ],
   "source": [
    "# Load source (English) and target (Pig Latin) words\n",
    "with open(\"data/small_src.txt\", \"r\") as f:\n",
    "    src_sentences = f.read().strip().split(\"\\n\")\n",
    "\n",
    "with open(\"data/small_tgt.txt\", \"r\") as f:\n",
    "    tgt_sentences = f.read().strip().split(\"\\n\")\n",
    "\n",
    "# Basic check\n",
    "print(f\"Loaded {len(src_sentences)} samples\")\n",
    "print(f\"Example:\\nSRC: {src_sentences[0]}\\nTGT: {tgt_sentences[0]}\")\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Get all characters from source and target\n",
    "all_chars = set(\"\".join(src_sentences + tgt_sentences))\n",
    "char2idx = {ch: idx + 4 for idx, ch in enumerate(sorted(all_chars))}\n",
    "char2idx[\"<pad>\"] = 0\n",
    "char2idx[\"<sos>\"] = 1\n",
    "char2idx[\"<eos>\"] = 2\n",
    "char2idx[\"<unk>\"] = 3\n",
    "\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "\n",
    "vocab_size = len(char2idx)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "def tokenize(sentence, char2idx, add_sos_eos=True):\n",
    "    tokens = [char2idx.get(c, char2idx[\"<unk>\"]) for c in sentence]\n",
    "    if add_sos_eos:\n",
    "        return [char2idx[\"<sos>\"]] + tokens + [char2idx[\"<eos>\"]]\n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "# Example:\n",
    "src_indices = tokenize(src_sentences[0], char2idx)\n",
    "tgt_indices = tokenize(tgt_sentences[0], char2idx)\n",
    "\n",
    "print(\"SRC indices:\", src_indices)\n",
    "print(\"TGT indices:\", tgt_indices)\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PigLatinDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, char2idx):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.char2idx = char2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = tokenize(self.src_sentences[idx], self.char2idx)\n",
    "        tgt = tokenize(self.tgt_sentences[idx], self.char2idx)\n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_lens = [len(seq) for seq in src_batch]\n",
    "    tgt_lens = [len(seq) for seq in tgt_batch]\n",
    "    src_padded = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=char2idx[\"<pad>\"])\n",
    "    tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=char2idx[\"<pad>\"])\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "dataset = PigLatinDataset(src_sentences, tgt_sentences, char2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize your transformer model\n",
    "encoder = Transformer_Encoder(N=6, d_model=512, n_head=8, vocab_size=vocab_size)\n",
    "decoder = Transformer_Decoder(d_model=512, num_heads=8, num_layers=6, vocab_size=vocab_size)\n",
    "model = TransformerEncoderDecoder(encoder, decoder, d_model=512, vocab_size=vocab_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=char2idx[\"<pad>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "device = next(model.parameters()).device\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src_batch, tgt_batch in dataloader:\n",
    "        src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
    "        tgt_input = tgt_batch[:, :-1]\n",
    "        tgt_target = tgt_batch[:, 1:]\n",
    "\n",
    "        # Causal mask for decoder\n",
    "        batch_size, tgt_seq_len = tgt_input.shape\n",
    "        tgt_mask = torch.tril(torch.ones(tgt_seq_len, tgt_seq_len, device=device)).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(src_batch, tgt_input, src_mask=None, tgt_mask=tgt_mask)\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        tgt_target = tgt_target.reshape(-1)\n",
    "\n",
    "        loss = criterion(logits, tgt_target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(dataloader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97009e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: chimney-board\n",
      "Pig Latin: imneychay-oardray\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(model, src_sentence, char2idx, idx2char, max_len=20, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize source sentence\n",
    "    src_tokens = [char2idx.get(c, char2idx[\"<unk>\"]) for c in src_sentence]\n",
    "    src_tokens = [char2idx[\"<sos>\"]] + src_tokens + [char2idx[\"<eos>\"]]\n",
    "    src_tensor = torch.tensor(src_tokens).unsqueeze(0).to(device)  # (1, src_seq_len)\n",
    "\n",
    "    # Encoder output\n",
    "    encoder_output = model.encoder(src_tensor)\n",
    "\n",
    "    # Start decoding with <sos>\n",
    "    tgt_tokens = [char2idx[\"<sos>\"]]\n",
    "    for _ in range(max_len):\n",
    "        tgt_input = torch.tensor(tgt_tokens).unsqueeze(0).to(device)  # (1, current_tgt_len)\n",
    "\n",
    "        # Causal mask for decoder\n",
    "        tgt_seq_len = tgt_input.shape[1]\n",
    "        tgt_mask = torch.tril(torch.ones(tgt_seq_len, tgt_seq_len, device=device)).unsqueeze(0)\n",
    "\n",
    "        # Decoder output\n",
    "        decoder_output = model.decoder(tgt_input, encoder_output, mask=tgt_mask)\n",
    "        logits = model.output_projection(decoder_output)  # (1, tgt_seq_len, vocab_size)\n",
    "\n",
    "        # Get the last time step prediction\n",
    "        next_token_logits = logits[0, -1, :]  # (vocab_size,)\n",
    "        next_token = torch.argmax(next_token_logits).item()\n",
    "\n",
    "        # If <eos>, stop decoding\n",
    "        if next_token == char2idx[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "        # Append next token\n",
    "        tgt_tokens.append(next_token)\n",
    "\n",
    "    # Convert token IDs back to string\n",
    "    output_chars = [idx2char[token] for token in tgt_tokens[1:]]  # Exclude <sos>\n",
    "    output_word = \"\".join(output_chars)\n",
    "    return output_word\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Example English word\n",
    "src_sentence = \"chimney-board\"\n",
    "\n",
    "# Translate to Pig Latin\n",
    "piglatin_translation = greedy_decode(model, src_sentence, char2idx, idx2char, device=device)\n",
    "\n",
    "print(f\"English: {src_sentence}\")\n",
    "print(f\"Pig Latin: {piglatin_translation}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
